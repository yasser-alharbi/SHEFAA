# -*- coding: utf-8 -*-
"""Simple-baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FTJKurgnXbzYUnkKUKJnUoaty_ByCCR6

# SHEFAA - Milestone 2
**Overview:**

In this simple baseline in Milestone 2, We begin by importing essential libraries for data manipulation, visualization, and modeling, including pandas, matplotlib, and scikit-learn. The dataset used was preprocessed and prepare it for analysis. The data was then split into training, vaildation, and testing sets, followed by the implementation of a baseline model. Finally, the model's performance was evaluated using standard metrics for text generation such as, BLEU, ROUGE, and BERTScore to provide a foundation for future improvements.

**Group Members:**
1.   Yasser Alharbi
1.   Eyad Alatifi
2.   Abdullah Alharbi
2.   Nawaf Alandijany
5.   Abdulaziz Abutaleb

Group 1

# Baseline (TF-IDF)
**A Simple TF-IDF Retrieval Baseline.**

**Description:**
This baseline model employs TF-IDF to convert both training and validation questions into numerical vectors reflecting their term frequency and relevance across the dataset. For each new question in the valid set, its TF-IDF vector is compared (via cosine similarity) against all training question vectors to identify the most similar one. The answer corresponding to this highest-similarity training question is then returned as the prediction.

**Steps:**

For each test question, we:
1.   Compute its TF-IDF vector.
2.   Compare it (via cosine similarity) with **all training questions**.
3.   Find the **closest** training question (the one with the highest similarity).
4.   **Retrieve** that training **question’s** answer as the predicted answer.

# Libraries
"""

pip install rouge-score

pip install bert-score

import numpy as np
import pandas as pd
import kagglehub
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# Evaluation
import nltk
nltk.download('punkt_tab')
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from bert_score import score
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

"""# 1- Dataset

### Data Exportation
"""

# Collecting the dataset
path = kagglehub.dataset_download("abdoashraf90/ahqad-arabic-healthcare-q-and-a-dataset")

df = pd.read_csv(path + '/AHQAD.csv')

# sample of the dataset
df.sample(5)

df.shape

"""## Data Cleaning

### Drop unnecessary feature
"""

df.drop(columns=['Unnamed: 0'], inplace=True)

"""### Exploring missing values:"""

# Looking for missing values in the dataset
df.isnull().sum()

# we can see that there are 174 missing answer of each question
# Missing answers:
df[df['Question'].isnull()]

# They will be dropped and the remaining dataset as follows:
df.dropna(inplace=True)
df.shape

# Looking for missing values in the dataset
df.isnull().sum()

"""### Duplicated rows"""

df.duplicated().sum()

df['Question'].duplicated().sum()

df['Answer'].duplicated().sum()

df[df['Question'].duplicated()]

"""### Drop duplicated rows"""

df.drop(df[df['Question'].duplicated()].index, inplace=True)

df.drop(df[df['Answer'].duplicated()].index, inplace=True)

df.shape

"""### Normalizing the "question/answer" columns

"""

print(f"Question:{df.iloc[1, 0]}")
print(f"Answer:{df.iloc[1, 1]}")

# Function to clean the Question and Answer columns in the dataset
def clean_question_and_answer_columns(dataset):
    # Strip leading and trailing whitespace from 'Question' and 'Answer' columns
    dataset['Question'] = dataset['Question'].str.strip()
    dataset['Answer'] = dataset['Answer'].str.strip()

    # Remove punctuation and special characters (?, !, ،, ;, :, (), [], {}, etc.)
    dataset['Question'] = dataset['Question'].str.replace(r'[^\w\s]', '', regex=True)
    dataset['Answer'] = dataset['Answer'].str.replace(r'[^\w\s]', '', regex=True)

    # Replace all types of whitespace (spaces, tabs, newlines) with a single space in one pass
    dataset['Question'] = dataset['Question'].str.replace(r'\s+', ' ', regex=True)
    dataset['Answer'] = dataset['Answer'].str.replace(r'\s+', ' ', regex=True)

    # Remove any content within square brackets (e.g., "[example]")
    dataset['Question'] = dataset['Question'].str.replace(r'[.?]', '', regex=True)
    dataset['Answer'] = dataset['Answer'].str.replace(r'[.?]', '', regex=True)

    # Normalize Arabic letters: Convert أ and إ to ا
    dataset['Question'] = dataset['Question'].str.replace(r'[أإ]', 'ا', regex=True)
    dataset['Answer'] = dataset['Answer'].str.replace(r'[أإ]', 'ا', regex=True)

    return dataset

# Call the function on the dataset
df = clean_question_and_answer_columns(df)

# after cleaning the pairs
print(f"Question:{df.iloc[1, 0]}")
print(f"Answer:{df.iloc[1, 1]}")

# taking some random pairs
r = np.random.randint(0, len(df))
print(f"Question:{df.iloc[r, 0]}")
print(f"Answer:{df.iloc[r, 1]}")

"""## Data Visualization"""

# font_path='/content/Amiri-Bold.ttf'

# # Generate a word cloud for the questions
# wordcloud = WordCloud(
#     font_path=font_path,
#     width=800,
#     height=400,
#     background_color='white',
#     colormap='viridis',
# ).generate(" ".join(df['Question']))

# plt.figure(figsize=(12, 6))
# plt.imshow(wordcloud, interpolation='bilinear')
# plt.axis('off')
# plt.title("Word Cloud for Questions")
# plt.show()

# # Generate a word cloud for the answers
# wordcloud = WordCloud(
#     font_path=font_path,
#     width=800,
#     height=400,
#     background_color='white',
#     colormap='plasma',
# ).generate(" ".join(df['Answer']))

# plt.figure(figsize=(12, 6))
# plt.imshow(wordcloud, interpolation='bilinear')
# plt.axis('off')
# plt.title("Word Cloud for Answers")
# plt.show()

"""## Data split (changeable)"""

# Creating a new column for both question + label named input_text which the model will learns
df['input_text'] = df.apply(
    lambda row: f"سؤال: {row['Question']} | التصنيف: {row['Category']}",
    axis=1
)
df['Answer'] = df.apply(lambda row: f'الإجابة: {row["Answer"]}', axis=1)
X= df['input_text']
y= df['Answer']

# sample
print(X[809])
print(y[809])

# Spliting the data into:
# Train 80% | Test 10% | Dev 10%
X_train , X_test_valid , y_train , y_test_valid = train_test_split(X,y ,test_size=0.2, random_state=42)
X_test , X_valid , y_test , y_valid = train_test_split(X_test_valid, y_test_valid, test_size=0.5, random_state=42)

# displaying the shapes
print(f"Train shape: {X_train.shape}")
print(f"Test shape: {X_test.shape}")
print(f"Val shape: {X_valid.shape}")

"""## Data Uploading"""

# Uploading the cleaned dataset
df.to_csv('cleaned_ArabicMQA_dataset.csv', index=False)

# Upload the training set as 1 file
df_train = pd.concat([X_train, y_train], axis=1)
df_train.to_csv('train.csv', index=False)

# Upload the testing set as 1 file
df_test = pd.concat([X_test, y_test], axis=1)
df_test.to_csv('test.csv', index=False)

# Upload the validation set as 1 file
df_valid = pd.concat([X_valid, y_valid], axis=1)
df_valid.to_csv('valid.csv', index=False)

"""# 2- Baseline

In case of our dataset X_valid is 71,519 which require more time and computationally expensive, we will take just a sample of 1,000 X_valid.
---

"""

# Build TF-IDF on the training question
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)

# The sample of the X_valid
y_valid_sample = y_valid[:1000]

# For each test question, find the most similar training question & retrieve its answer
y_pred = []
for test_question in X_valid[:1000]:
    # Transform test question into TF-IDF vector
    test_question_tfidf = vectorizer.transform([test_question])
    # Compute cosine similarity with all training questions
    similarities = cosine_similarity(test_question_tfidf, X_train_tfidf)
    # Get the index of the most similar training question
    best_index = np.argmax(similarities)
    # Retrieve the corresponding answer
    predicted_answer = y_train.iloc[best_index]
    y_pred.append(predicted_answer)

print("y_pred shape", len(y_pred))
print("y_valid shape", len(y_valid_sample))

print(f"Test question: \n{X_valid.iloc[50]}")
print(f"Predicted answer: \n{y_pred[50]}")
print(f"True answer: \n{y_valid_sample.iloc[50]}")

"""## Uploading the reference/prediction sample in .txt"""

# Uploading the reference sample in .txt
with open('1000_Sample_Reference.txt', 'w') as f:
    for answer in y_valid_sample:
        f.write(answer + '\n')

# Uploading the prediction sample in .txt
with open('1000_Sample_Prediction.txt', 'w') as f:
    for answer in y_pred:
        f.write(answer + '\n')

"""# 3- Evaluation

##1. BLEU (Using nltk)
"""

# Creating BELU score function
# Computes sentence-level BLEU for each prediction-reference pair and returns a list of BLEU scores.
def compute_bleu_scores(references, predictions):
    smoothie = SmoothingFunction().method4  # Smooth to avoid zero scores for short texts
    bleu_scores = {"bleu_1": [], "bleu_2": [], "bleu_4": []}  # Store scores for each level


    for ref, hyp in zip(references, predictions):
        # Tokenize
        ref_tokens = nltk.word_tokenize(ref)
        hyp_tokens = nltk.word_tokenize(hyp)
        # BLEU expects a list of reference lists: [[ref_tokens]]
        bleu_1 = sentence_bleu([ref_tokens], hyp_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)
        bleu_2 = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)
        bleu_4 = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)

        # Append scores
        bleu_scores["bleu_1"].append(bleu_1)
        bleu_scores["bleu_2"].append(bleu_2)
        bleu_scores["bleu_4"].append(bleu_4)

    return bleu_scores
bleu_scores = compute_bleu_scores(y_valid_sample, y_pred)

# Displaying the accuracy
bleu_1_avg = sum(bleu_scores["bleu_1"]) / len(bleu_scores["bleu_1"])
bleu_2_avg = sum(bleu_scores["bleu_2"]) / len(bleu_scores["bleu_2"])
bleu_4_avg = sum(bleu_scores["bleu_4"]) / len(bleu_scores["bleu_4"])

print(f"Average BLEU-1 accuracy: {bleu_1_avg:.3f}")
print(f"Average BLEU-2 accuracy: {bleu_2_avg:.3f}")
print(f"Average BLEU-4 accuracy: {bleu_4_avg:.3f}")

"""##2. ROUGE (Using rouge-score)

"""

# Creating ROUGE score function
# Computes ROUGE-1, ROUGE-2, and ROUGE-L for each prediction-reference pair, then returns a list of dicts with these scores.
def compute_rouge_scores(references, predictions):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    results = []
    for ref, hyp in zip(references, predictions):
        scores = scorer.score(ref, hyp)
        results.append(scores)
    return results
rouge_results = compute_rouge_scores(y_valid_sample, y_pred)

# Displaying the accuracy
avg_rouge1 = sum(r['rouge1'].fmeasure for r in rouge_results) / len(rouge_results)
avg_rouge2 = sum(r['rouge2'].fmeasure for r in rouge_results) / len(rouge_results)
avg_rougeL = sum(r['rougeL'].fmeasure for r in rouge_results) / len(rouge_results)
print(f"Average ROUGE-1 F1: {(avg_rouge1):.3f}")
print(f"Average ROUGE-2 F1: {(avg_rouge2):.3f}")
print(f"Average ROUGE-L F1: {(avg_rougeL):.3f}")

"""##3. BERTScore (Using bert-score)

"""

# Creating BERT score function
# Computes BERTScore (Precision, Recall, F1) for each prediction-reference pair.
# For Arabic, we set lang='ar' which uses a multilingual model by default.
def compute_bertscore(references, predictions, lang="ar"):
    references = references.tolist()
    # precision, recall, F1
    P, R, F1 = score(predictions, references, lang=lang)
    return P, R, F1

# Displaying the accuracy
P, R, F1 = compute_bertscore(y_valid_sample, y_pred, lang='ar')

# Displaying the accuracy
print(f"Average BERTScore Precision: {(P.mean() * 100):.2f}%")
print(f"Average BERTScore Recall: {(R.mean() * 100):.2f}%")
print(f"Average BERTScore F1: {(F1.mean() * 100):.2f}%")