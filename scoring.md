# Evaluation Metrics in the SHAFAA Project

## Introduction
To comprehensively evaluate the SHAFAA Arabic Medical Question Answering system, we employ a combination of precision-based and semantic evaluation metrics. These metrics assess the system's ability to generate accurate, contextually appropriate, and semantically coherent answers. The evaluation includes BLEU, ROUGE, and BERTScore, offering a multi-faceted perspective on the system's performance.

---

## Evaluation Metrics

### 1. **BLEU (Bilingual Evaluation Understudy)**
- **Description**: BLEU measures n-gram precision by comparing the overlap of n-grams between the system's predictions and the reference answers.
- **Variants**:
  - **BLEU-1**: Unigram overlap.
  - **BLEU-2**: Bigram overlap.
  - **BLEU-3**: Trigram overlap.
- **Purpose**: Measures lexical overlap and phrase-level accuracy in generated responses.
- **Implementation**: Calculated using the NLTK library with smoothing techniques to handle short text scenarios.

---

### 2. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**
- **Description**: ROUGE emphasizes recall, evaluating how much of the reference text is covered by the generated text.
- **Variants**:
  - **ROUGE-1**: Unigram overlap.
  - **ROUGE-2**: Bigram overlap.
  - **ROUGE-L**: Longest Common Subsequence (LCS), evaluating in-order matches.
- **Purpose**: Highlights both word-level and sequence-level similarities, providing insights into the coherence of generated answers.
- **Implementation**: Calculated using the `rouge_score` library.

---

### 3. **BERTScore**
- **Description**: BERTScore uses pretrained transformer embeddings (e.g., BERT) to compare the semantic similarity between generated and reference texts. Instead of exact word matching, it computes cosine similarity between token embeddings.
- **Metrics**:
  - **Precision**: Measures how well the generated text captures relevant parts of the reference.
  - **Recall**: Measures how much of the reference text is reflected in the generated output.
  - **F1-Score**: Balances precision and recall to provide an overall performance score.
- **Implementation**:
  - Leverages the `bert-score` library with multilingual model support (`lang='ar'`) for Arabic text.
  - Example usage:
    ```python
    P, R, F1 = compute_bertscore(y_valid_sample, y_pred, lang='ar')
    print(f"Average BERTScore Precision: {P.mean() * 100:.2f}%")
    print(f"Average BERTScore Recall: {R.mean() * 100:.2f}%")
    print(f"Average BERTScore F1: {F1.mean() * 100:.2f}%")
    ```
- **Purpose**: Evaluates semantic alignment between generated and reference answers, making it ideal for tasks where exact word matching is insufficient.

---

### 4. **BARTScore**
- **Description**: BARTScore is a generation-based metric that evaluates the quality of text by scoring how well it can be generated by a pretrained BART model. It measures the fluency and coherence of the generated text by comparing its likelihood under the model.
- **Variants**:
  - **Precision (Generation)**: Measures how likely the reference text can generate the candidate text.
  - **Recall (Paraphrase)**: Measures how likely the candidate text can generate the reference text.
  - **F1-Score**: Combines precision and recall for a balanced score.
- **Implementation**:
  - BARTScore leverages pretrained BART models fine-tuned for Arabic or multilingual tasks.
  - Example usage:
    ```python
    from bart_score import BARTScorer

    # Load the BART model
    scorer = BARTScorer(device='cuda', checkpoint='facebook/bart-large-cnn')

    # Compute scores
    precision = scorer.score(y_pred, y_valid_sample, avg=True)
    recall = scorer.score(y_valid_sample, y_pred, avg=True)
    f1_score = (2 * precision * recall) / (precision + recall)
    print(f"Average BARTScore Precision: {precision:.2f}")
    print(f"Average BARTScore Recall: {recall:.2f}")
    print(f"Average BARTScore F1: {f1_score:.2f}")
    ```
- **Purpose**: Assesses fluency, coherence, and overall quality of generated answers using generative model probabilities.

---

## Summary of Metrics

| Metric     | Precision | Recall | F1-Score | Purpose                               |
|------------|-----------|--------|----------|---------------------------------------|
| BLEU       | ✅         | ❌      | ❌        | Lexical overlap and phrase-level accuracy. |
| ROUGE      | ✅         | ✅      | ✅        | Word-level and sequence-level recall. |
| BERTScore  | ✅         | ✅      | ✅        | Semantic similarity using embeddings. |
| BARTScore  | ✅         | ✅      | ✅        | Fluency, coherence, and generation quality. |

---

## Conclusion
The combination of BLEU, ROUGE, BERTScore, and BARTScore ensures a comprehensive evaluation of the SHAFAA system. Each metric highlights different aspects of the system’s performance, from lexical and semantic accuracy to fluency and coherence. This multi-faceted evaluation helps identify strengths and areas for improvement in the model.

